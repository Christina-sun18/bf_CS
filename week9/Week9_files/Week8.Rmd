---
title: "Exercise solutions: Section 9.11"
author: "Rob J Hyndman and George Athanasopoulos"
output:
  bookdown::html_document2:
    fig_height: 5
    fig_width: 8
    toc: yes
    toc_depth: 1
    toc_float:
      collapsed: false
    number_sections: false
    code_folding: show
    theme: readable
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, cache = TRUE)
library(fpp3)
library(patchwork)
```


# fpp3 9.11, Ex6

> Simulate and plot some data from simple ARIMA models.
>   a. Use the following R code to generate data from an AR(1) model with $\phi_{1} = 0.6$ and $\sigma^2=1$. The process starts with $y_1=0$.

```{r ex6a}
ar1 <- function(phi, n = 100L) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- phi * y[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
```

>   b. Produce a time plot for the series. How does the plot change as you change $\phi_1$?

Some examples of changing $\phi_1$

```{r ex6b}
ar1(0.6)
ar1(0.6) %>% autoplot(y) + labs(title=expression(paste(phi, "=0.6")))
ar1(0.95) %>% autoplot(y) + labs(title=expression(paste(phi, "=0.95")))
ar1(0.05) %>% autoplot(y) + labs(title=expression(paste(phi, "=0.05")))
ar1(-0.65) %>% autoplot(y) + labs(title=expression(paste(phi, "=-0.65")))
```

>   c. Write your own code to generate data from an MA(1) model with $\theta_{1}  =  0.6$ and $\sigma^2=1$.

```{r ex6c}
ma1 <- function(theta, n = 100L) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- theta * e[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
```

>   d. Produce a time plot for the series. How does the plot change as you change $\theta_1$?

```{r ex6d}
ma1(0.6) %>% autoplot(y) + labs(title=expression(paste(theta, "=0.6")))
ma1(0.95) %>% autoplot(y) + labs(title=expression(paste(theta, "=0.95")))
ma1(0.05) %>% autoplot(y) + labs(title=expression(paste(theta, "=0.05")))
ma1(-0.65) %>% autoplot(y) + labs(title=expression(paste(theta, "=-0.65")))

```

>   e. Generate data from an ARMA(1,1) model with $\phi_{1} = 0.6$, $\theta_{1}  = 0.6$ and $\sigma^2=1$.

```{r ex6e}
arma11 <- function(phi, theta, n = 100) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- phi * y[i - 1] + theta * e[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
arma11(0.6, 0.6) %>% autoplot(y)
```

>   f. Generate data from an AR(2) model with $\phi_{1} =-0.8$, $\phi_{2} = 0.3$ and $\sigma^2=1$. (Note that these parameters will give a non-stationary series.)

```{r ex6f}
ar2 <- function(phi1, phi2, n = 100) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 3:n) {
    y[i] <- phi1 * y[i - 1] + phi2 * y[i - 2] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
ar2(-0.8, 0.3) %>% autoplot(y)
```

>   g. Graph the latter two series and compare them.

See graphs above. The non-stationarity of the AR(2) process has led to increasing oscillations

# fpp3 9.11, Ex7

> Consider `aus_airpassengers`, the total number of passengers (in millions) from Australian air carriers for the period 1970-2011.

>   a. Use `ARIMA()` to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.

```{r ex7a}
aus_airpassengers %>% autoplot(Passengers)
fit <- aus_airpassengers %>% 
  model(arima = ARIMA(Passengers))
report(fit)
fit %>% gg_tsresiduals()
fit %>% forecast(h = 10) %>% autoplot(aus_airpassengers)
```

>    b. Write the model in terms of the backshift operator.

```{r ex7b, echo=FALSE}
theta <- tidy(fit) %>% pull(estimate)
sigma2 <- glance(fit) %>% pull(sigma2)
sigma2
```

$$(1-B)^2y_t = (1+\theta B)\varepsilon_t$$
where $\varepsilon\sim\text{N}(0,\sigma^2)$, $\theta = `r sprintf("%.2f",theta)`$ and $\sigma^2 = `r sprintf("%.2f",sigma2)`$.

>    c. Plot forecasts from an ARIMA(0,1,0) model with drift and compare these to part a.

```{r ex7c}
aus_airpassengers %>% 
  model(arima = ARIMA(Passengers ~ 1 + pdq(0,1,0))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)
```

- Both containing increasing trends, but the ARIMA(0,2,1) model has an implicit trend due to the double-differencing, while the ARIMA(0,1,0) with drift models the trend directly via the trend coefficient. 
- The intervals are narrower when there are fewer differences.

>    d. Plot forecasts from an ARIMA(2,1,2) model with drift and compare these to part b. Remove the constant and see what happens.

```{r ex7d}
aus_airpassengers %>% 
  model(arima = ARIMA(Passengers ~ 1 + pdq(2,1,2))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)
aus_airpassengers %>% 
  model(arima = ARIMA(Passengers ~ 0 + pdq(2,1,2)))
```

-   There is little difference between ARIMA(2,1,2) with drift and
    ARIMA(0,1,0) with drift.
-   Removing the constant causes an error because the model cannot be
    estimated.

>    e. Plot forecasts from an ARIMA(0,2,1) model with a constant. What happens?

```{r ex7e, warning=TRUE, message=TRUE}
aus_airpassengers %>% 
  model(arima = ARIMA(Passengers ~ 1 + pdq(0,2,1))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)
```

The forecast trend is now quadratic, and there is a warning that this is generally a bad idea.

# fpp3 9.11, Ex8

> For the United States GDP series (from `global_economy`):
>
>  a. If necessary, find a suitable Box-Cox transformation for the data;

```{r ex8a}
us_economy <- global_economy %>%
  filter(Code == "USA")
us_economy %>%
  autoplot(GDP)

lambda <- us_economy %>%
  features(GDP, features = guerrero) %>%
  pull(lambda_guerrero)
lambda

us_economy %>%
  autoplot(box_cox(GDP, lambda))
```
It seems that a Box-Cox transformation may be useful here.

>  b. fit a suitable ARIMA model to the transformed data using `ARIMA()`;

```{r ex8b}
fit <- us_economy %>%
  model(ARIMA(box_cox(GDP, lambda)))
report(fit)
```

>  c. try some other plausible models by experimenting with the orders chosen;

```{r ex8c}
fit <- us_economy %>%
  model(
    arima010 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 0)),
    arima011 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 1)),
    arima012 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 2)),
    arima013 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(0, 1, 3)),
    arima110 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 0)),
    arima111 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 1)),
    arima112 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 2)),
    arima113 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 3)),
    arima210 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 0)),
    arima211 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 1)),
    arima212 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 2)),
    arima213 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(2, 1, 3)),
    arima310 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 0)),
    arima311 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 1)),
    arima312 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 2)),
    arima313 = ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(3, 1, 3))
  )
```

>  d. choose what you think is the best model and check the residual diagnostics;

```{r ex8d}
fit %>%
  glance() %>%
  arrange(AICc) %>%
  select(.model, AICc)
```

The best according to the AICc values is the `r model_sum(fit[[glance(fit)[[".model"]][[which.min(glance(fit)$AICc)]]]][[1]])` model.

```{r ex8d2} 
best_fit <- us_economy %>%
  model(ARIMA(box_cox(GDP, lambda) ~ 1 + pdq(1, 1, 0)))
best_fit %>% report()
best_fit %>% gg_tsresiduals()
augment(best_fit) %>% features(.innov, ljung_box, dof = 2, lag = 10)
```

The residuals pass the Ljung-Box test, but the histogram looks like negatively skewed.

>  e. produce forecasts of your fitted model. Do the forecasts look reasonable?

```{r ex8e}
best_fit %>%
  forecast(h = 10) %>%
  autoplot(us_economy)
```

These look reasonable. Let's compare a model with no transformation.

```{r ex8e2}
fit1 <- us_economy %>% model(ARIMA(GDP))
fit1 %>%
  forecast(h = 10) %>%
  autoplot(us_economy)
```

Notice the effect of the transformation on the forecasts. Increase the forecast horizon to  see what happens. Notice also the width of the prediction intervals.

```{r ex8e3}
us_economy %>%
  model(
    ARIMA(GDP),
    ARIMA(box_cox(GDP, lambda))
  ) %>%
  forecast(h = 100) %>%
  autoplot(us_economy)
```

>  f. compare the results with what you would obtain using `ETS()` (with no transformation).

```{r ex8f}
us_economy %>%
  model(ETS(GDP)) %>%
  forecast(h = 10) %>%
  autoplot(us_economy)
```

The point forecasts are similar, however the ETS forecast intervals are much wider.

# fpp3 9.11, Ex15

> Consider the number of Snowshoe Hare furs traded by the Hudson Bay Company between 1845 and 1935 (data set `pelt`).

> a. Produce a time plot of the time series.

```{r}
pelt %>% autoplot(Hare)
```

> b. Assume you decide to fit the following model: 
> $$ y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \phi_3 y_{t-3} + \phi_4 y_{t-4} + \varepsilon_t, $$
> where $\varepsilon_t$ is a white noise series. What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

- This is an ARIMA(4,0,0), hence $p=4$, $d=0$ and $q=0$.

> c. By examining the ACF and PACF of the data, explain why this model is appropriate.


```{r}
pelt %>% gg_tsdisplay(plot="partial")
fit <- pelt %>% model(AR4 = ARIMA(Hare ~ pdq(4,0,0)))
fit %>% gg_tsresiduals()
```

- The significant spike at lag 4 of the PACF indicates an AR(4). 
- The residuals from this model are clearly whhite noise. 

>    d. The last five values of the series are given below:

```{r hares, echo=FALSE, warning=FALSE, message=FALSE}
pelt_table <- pelt %>%
  tail(5) %>%
  select(Year, Hare)
tab <- as.data.frame(matrix(c(NA, pelt_table$Hare), nrow = 1))
colnames(tab) <- c("Year", pelt_table$Year)
tab[1, 1] <- "Number of hare pelts"
tab %>%
  knitr::kable()

fit <- pelt %>% model(ARIMA(Hare ~ pdq(4, 0, 0)))
coef <- rlang::set_names(tidy(fit)$estimate, tidy(fit)$term)
constant <- coef["constant"]
phi1 <- coef["ar1"]
phi2 <- coef["ar2"]
phi3 <- coef["ar3"]
phi4 <- coef["ar4"]
fc1 <- constant + sum(c(phi1,phi2,phi3,phi4)*pelt_table$Hare[5:2])
fc2 <- constant + sum(c(phi1,phi2,phi3,phi4)*c(fc1,pelt_table$Hare[5:3]))
fc3 <- constant + sum(c(phi1,phi2,phi3,phi4)*c(fc2,fc1,pelt_table$Hare[5:4]))
```

>   The estimated parameters are
>    $c = `r sprintf("%.0f",constant)`$,
>    $\phi_1 = `r sprintf("%.2f", phi1)`$,
>    $\phi_2 = `r sprintf("%.2f", phi2)`$,
>    $\phi_3 = `r sprintf("%.2f", phi3)`$, and
>    $\phi_4 = `r sprintf("%.2f", phi4)`$.
>    Without using the `forecast` function, calculate forecasts for the next three years (1936--1939).

\begin{align*}
  \hat{y}_{T+1|T} & = `r sprintf("%.0f",constant)` + 
    `r sprintf("%.2f", phi1)`* `r sprintf("%.0f",pelt_table$Hare[5])`  
    `r sprintf("%.2f", phi2)`* `r sprintf("%.0f",pelt_table$Hare[4])`  
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[3])`  
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[2])` =
    `r sprintf("%.2f", fc1)` \\
  \hat{y}_{T+2|T} & = `r sprintf("%.0f",constant)` + 
    `r sprintf("%.2f", phi1)`* `r sprintf("%.2f",fc1)`  
    `r sprintf("%.2f", phi2)`* `r sprintf("%.0f",pelt_table$Hare[5])`  
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[4])`  
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[3])` =
    `r sprintf("%.2f", fc2)` \\
  \hat{y}_{T+3|T} & = `r sprintf("%.0f",constant)` + 
    `r sprintf("%.2f", phi1)`* `r sprintf("%.2f",fc2)`  
    `r sprintf("%.2f", phi2)`* `r sprintf("%.2f",fc1)`  
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[5])`  
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[4])` =
    `r sprintf("%.2f", fc3)` \\
\end{align*}

>    e. Now fit the model in R and obtain the forecasts using `forecast`. How are they different from yours? Why?

```{r ex15e}
pelt %>% 
  model(ARIMA(Hare ~ pdq(4, 0, 0))) %>%
  forecast(h=3)
```

Any differences will be due to rounding errors.

# fpp3 9.11, Ex16

> The population of Switzerland from 1960 to 2017 is in data set `global_economy`.

> a. Produce a time plot of the data.

```{r}
swiss_pop <- global_economy %>%
  filter(Country == "Switzerland") %>%
  select(Year, Population) %>%
  mutate(Population = Population / 1e6)

autoplot(swiss_pop, Population)
```

> b. You decide to fit the following model to the series:
> $$y_t = c + y_{t-1} + \phi_1 (y_{t-1} - y_{t-2}) + \phi_2 (y_{t-2} - y_{t-3}) + \phi_3( y_{t-3} - y_{t-4}) + \varepsilon_t$$
> where $y_t$ is the Population in year $t$ and $\varepsilon_t$ is a white noise series. What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

This is an ARIMA(3,1,0), hence $p=3$, $d=1$ and $q=0$.


> c. Explain why this model was chosen using the ACF and PACF of the differenced series.

```{r}
swiss_pop %>% gg_tsdisplay(Population, plot="partial")
```

```{r}
swiss_pop %>% gg_tsdisplay(difference(Population), plot="partial")
```

The significant spike at lag 3 in the PACF, coupled with the  exponential decay in the ACF, for the differenced series, signals an AR(3) for the differenced series.

> d. The last five values of the series are given below.

```{r swisspop, echo=FALSE, warning=FALSE, message=FALSE}
swiss_pop <- global_economy %>%
  filter(Country == "Switzerland") %>%
  tail(5) %>%
  select(Year, Population) %>%
  mutate(Population = Population / 1e6)
tab <- as.data.frame(matrix(c(NA, swiss_pop$Population), nrow = 1))
colnames(tab) <- c("Year", swiss_pop$Year)
tab[1, 1] <- "Population (millions)"
tab %>%
  knitr::kable(digits = 2)
fit <- global_economy %>%
  filter(Country == "Switzerland") %>%
  mutate(Population = Population / 1e6) %>%
  model(ARIMA(Population ~ 1 + pdq(3, 1, 0)))
coef <- rlang::set_names(tidy(fit)$estimate, tidy(fit)$term)
phi1 <- coef["ar1"]
phi2 <- coef["ar2"]
phi3 <- coef["ar3"]
intercept <- coef["constant"]
fc1 <- intercept + swiss_pop$Population[5] + sum(c(phi1,phi2,phi3)*diff(swiss_pop$Population)[4:2])
fc2 <- intercept + fc1 + sum(c(phi1,phi2,phi3)*diff(c(swiss_pop$Population,fc1))[5:3])
fc3 <- intercept + fc2 + sum(c(phi1,phi2,phi3)*diff(c(swiss_pop$Population,fc1,fc2))[6:4])
```

> The estimated parameters are $c = `r sprintf("%.4f",intercept)`$, $\phi_1 = `r sprintf("%.2f", phi1)`$, $\phi_2 = `r sprintf("%.2f", phi2)`$, and $\phi_3 = `r sprintf("%.2f", phi3)`$. Without using the `forecast` function, calculate forecasts for the next three years (2018--2020).

\begin{align*}
  \hat{y}_{T+1|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",swiss_pop$Population[5])`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",swiss_pop$Population[4])` - `r sprintf("%.2f",swiss_pop$Population[3])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[3])` - `r sprintf("%.2f",swiss_pop$Population[2])`) =
    `r sprintf("%.2f", fc1)` \\
  \hat{y}_{T+2|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",fc1)`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",fc1)` - `r sprintf("%.2f",swiss_pop$Population[5])`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[4])` - `r sprintf("%.2f",swiss_pop$Population[3])`) =
    `r sprintf("%.2f", fc2)` \\
  \hat{y}_{T+3|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",fc2)`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",fc2)` - `r sprintf("%.2f",fc1)`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",fc1)` - `r sprintf("%.2f",swiss_pop$Population[5])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`) =
    `r sprintf("%.2f", fc3)` \\
\end{align*}

>   e. Now fit the model in R and obtain the forecasts from the same model. How are they different from yours? Why?
    
```{r ex16e}
global_economy %>%
  filter(Country == "Switzerland") %>%
  mutate(Population = Population / 1e6) %>%
  model(ARIMA(Population ~ 1 + pdq(3, 1, 0))) %>%
  forecast(h=3)
```

Any differences will be due to rounding errors.

# fpp3 9.11, Ex17

>    a. Select a time series from [Quandl](https://www.quandl.com). Then copy its short URL and import the data.

```{r ex17a}
y <- Quandl::Quandl("ODA/PBEVE_INDEX") %>%
  mutate(Month = yearmonth(Date)) %>%
  as_tsibble(index=Month)
```

>    b. Plot graphs of the data, and try to identify an appropriate ARIMA model.

```{r ex17b}
y %>% gg_tsdisplay(Value, plot_type="partial")
y %>% gg_tsdisplay(difference(Value), plot_type="partial")
```

Looks like an ARIMA(5,1,0) or an ARIMA(0,1,5)

```{r ex17b2}
fit <- y %>%
  model(
    arima510 = ARIMA(Value ~ pdq(5,1,0) + PDQ(0,0,0)),
    arima015 = ARIMA(Value ~ pdq(0,1,5) + PDQ(0,0,0)),
    auto = ARIMA(Value)
  )
glance(fit) %>% arrange(AICc)
```

My guessed ARIMA(0,1,5) is best.

>    c. Do residual diagnostic checking of your ARIMA model. Are the residuals white noise?

```{r ex17c}
fit %>%
  select(arima015) %>%
  gg_tsresiduals()
```

No obvious problems.

>    d. Use your chosen ARIMA model to forecast the next four years.

```{r ex17d}
fit %>%
  select(arima015) %>%
  forecast(h = "4 years") %>%
  autoplot(y)
```

>    e. Now try to identify an appropriate ETS model.

```{r ex17e}
y %>%
  model(ets = ETS(Value)) 
```

Automatic selection gives a damped additive trend.

>    f. Do residual diagnostic checking of your ETS model. Are the residuals white noise?


```{r ex17f}
y %>%
  model(ets = ETS(Value)) %>%
  gg_tsresiduals()
```

There is a large amount of autocorrelation at lag 5.

>    g. Use your chosen ETS model to forecast the next four years.

```{r ex17g}
y %>%
  model(ets = ETS(Value)) %>%
  forecast(h = "4 years") %>%
  autoplot(y)
```

The prediction intervals are much wider than the ARIMA model.

>    h. Which of the two models do you prefer?
 
I prefer the ARIMA model because it better captures the autocorrelations in the data, and has narrower prediction intervals.

